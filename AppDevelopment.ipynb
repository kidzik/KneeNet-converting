{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils\n",
    "from torch.utils.data import sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "from torch.autograd import Variable\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.ndimage\n",
    "import cv2\n",
    "import math\n",
    "import dill\n",
    "import pickle\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_preprocessing(image, \n",
    "                          min_hw_ratio = 1, \n",
    "                          output_width = 299, \n",
    "                          output_height = 299):\n",
    "\n",
    "    # Trim equal rows from top and bottom to get a square image\n",
    "    r, c = image.shape\n",
    "    image_hw_ratio = r/c\n",
    "    r_to_keep = c * min_hw_ratio\n",
    "    r_to_delete = r - r_to_keep\n",
    "    remove_from_top = int(math.ceil(r_to_delete/2))\n",
    "    remove_from_bottom = int(math.floor(r_to_delete/2))\n",
    "    image_top_bottom_trimmed = image[remove_from_top:(r-remove_from_bottom),:]\n",
    "\n",
    "    # resample to get the desired image size\n",
    "    image_resampled = cv2.resize(image_top_bottom_trimmed, dsize=(output_width, output_height), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Normalize pixel values to take the range [0,1]\n",
    "    image_clean = image_resampled - np.mean(image_resampled)\n",
    "    image_clean = image_clean / np.std(image_clean)\n",
    "    image_clean = ((image_clean - np.min(image_clean)) / (np.max(image_clean) - np.min(image_clean)))  \n",
    "\n",
    "    # Stack into three channels\n",
    "    image_clean_stacked = np.dstack((image_clean, image_clean, image_clean))\n",
    "    image_clean_stacked = np.moveaxis(image_clean_stacked, -1, 0)  \n",
    "    \n",
    "    # Implement ImageNet Standardization\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406]).reshape((3,1,1))\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225]).reshape((3,1,1))\n",
    "    image_clean_stacked = (image_clean_stacked - imagenet_mean) / imagenet_std\n",
    "    \n",
    "    return image_clean_stacked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1382. 1393. 1437. ...  252.  255.  227.]\n",
      " [1418. 1352. 1413. ...  257.  249.  220.]\n",
      " [1389. 1429. 1439. ...  235.  243.  256.]\n",
      " ...\n",
      " [1596. 1650. 1535. ...  919.  905.  803.]\n",
      " [1634. 1587. 1512. ...  926.  920.  788.]\n",
      " [1578. 1518. 1582. ...  912.  859.  813.]]\n",
      "(356.0, 356.0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-068c0ee6f11a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0msample_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0msample_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0msample_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-18b2edf74869>\u001b[0m in \u001b[0;36mdefault_preprocessing\u001b[0;34m(image, min_hw_ratio, output_width, output_height)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mremove_from_bottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_to_delete\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_from_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_from_bottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mimage_top_bottom_trimmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremove_from_top\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mremove_from_bottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# resample to get the desired image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "# Initialize a model\n",
    "pretrained_model = torchvision.models.densenet169(pretrained = True)\n",
    "\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "pretrained_model.aux_logits = False\n",
    "\n",
    "# Modify the output layer of the densenet to fit our number of output classses\n",
    "num_features = pretrained_model.classifier.in_features\n",
    "num_features_knee = 14976\n",
    "\n",
    "out_features = pretrained_model.classifier.out_features\n",
    "pretrained_model.classifier = nn.Linear(num_features_knee, 5)\n",
    "\n",
    "for param in pretrained_model.classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Load the model's trained weights\n",
    "pretrained_model.load_state_dict(torch.load('./DenseNet_classification_lr_0.0001_val_acc_0.6913822735084704_pretrained_True_epoch_6_currentLR_1e-05_DropOutProb_0.0', \n",
    "                                            map_location=lambda storage, \n",
    "                                            loc: storage))\n",
    "pretrained_model.train(False)\n",
    "pretrained_model = pretrained_model.double()\n",
    "\n",
    "# Save it into Onnx format for export to iOS\n",
    "from torch.autograd import Variable\n",
    "import torch.onnx\n",
    "\n",
    "# Create a sample image, which onnx needs\n",
    "sample_input = np.load('L.npy').astype('float') \n",
    "print(sample_input)\n",
    "sample_input = default_preprocessing(sample_input)\n",
    "\n",
    "sample_input = sample_input.reshape((1,sample_input.shape))\n",
    "sample_input = torch.from_numpy(sample_input)\n",
    "sample_input = sample_input.double()\n",
    "\n",
    "# Change the name of the first layer of the onnx model to \"data\"\n",
    "input_names = [ \"data\" ] \n",
    "\n",
    "torch.onnx._export(pretrained_model, \n",
    "                  sample_input, \n",
    "                  \"KneeDeep.onnx\", \n",
    "                   input_names = input_names,\n",
    "                   export_params=True,\n",
    "                  verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/lukasz/mri/kevin/ai_data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/media/lukasz/mri/kevin/ai_data/AppDevelopment.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
